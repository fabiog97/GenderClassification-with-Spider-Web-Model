{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo script i dati vengono preparati al training andandoli a bilanciare e normalizzare. Successivamente viene fatta un'operazione di split per creare train set e test set per ogni dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import di librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import dlib.cuda as cuda\n",
    "dlib.DLIB_USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione di conversione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione converte le stringhe in array di NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(instr):\n",
    "    return np.fromstring(instr[1:-1],sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lettura dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(\"dfCelebA.csv\", converters={'dizionario':converter})\n",
    "dataset_2 = pd.read_csv(\"dfWheDataset.csv\", converters={'dizionario':converter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione vengono concatenati i due dataset (dataset1 CelebA e dataset2 WheDataset) e poi vengono mescolati tra di loro tramite la funzione sample() in modo da creare un nuovo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [dataset_1, dataset_2]\n",
    "dataset_conc = pd.concat(frames)\n",
    "dataset3 = dataset_conc.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilanciamento dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione tutti e tre i dataset prima di essere suddivisi in train e test set vengono bilancianti andando a eguagliare il numero di vettori caratteristica che rappresentano un uomo e il numero di vettori caratteristica che rappresentano una donna. In questo modo su ogni dataset abbiamo lo stesso numero di uomini e di donne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilanciamento dataset1 - CelebA\n",
    "df1_majority = dataset_1[dataset_1.gender_str==0]\n",
    "df1_minority = dataset_1[dataset_1.gender_str==1]\n",
    "\n",
    "df1_majority_downsampled = resample(df1_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=84434,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "df1_downsampled = pd.concat([df1_majority_downsampled, df1_minority])\n",
    "dataset1_balanced = df1_downsampled.sample(frac=1)\n",
    "\n",
    "\n",
    "#bilanciamento dataset2 - WeDataset\n",
    "df2_majority = dataset_2[dataset_2.gender_str==1]\n",
    "df2_minority = dataset_2[dataset_2.gender_str==0]\n",
    "\n",
    "df2_majority_downsampled = resample(df2_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=36451,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "df2_downsampled = pd.concat([df2_majority_downsampled, df2_minority])\n",
    "dataset2_balanced = df2_downsampled.sample(frac=1)\n",
    "\n",
    "\n",
    "\n",
    "#bilanciamento dataset3 - WeDataset + CelebA\n",
    "df3_majority = dataset3[dataset3.gender_str==0]\n",
    "df3_minority = dataset3[dataset3.gender_str==1]\n",
    "\n",
    "df3_majority_downsampled = resample(df3_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=148169,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "df3_downsampled = pd.concat([df3_majority_downsampled, df3_minority])\n",
    "dataset3_balanced = df3_downsampled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizzazione dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizzazione dataset1 - CelebA\n",
    "X1 = dataset1_balanced.dizionario.to_numpy()\n",
    "X1 = np.stack(X1)\n",
    "X1 = normalize(X1)\n",
    "y1 = dataset1_balanced.gender_str.to_numpy()\n",
    "\n",
    "#Normalizzazione dataset2 - WheDataset\n",
    "X2 = dataset2_balanced.dizionario.to_numpy()\n",
    "X2 = np.stack(X2)\n",
    "X2 = normalize(X2)\n",
    "y2 = dataset2_balanced.gender_str.to_numpy()\n",
    "\n",
    "#Normalizzazione dataset3 - CelebA + WheDataset\n",
    "X3 = dataset3_balanced.dizionario.to_numpy()\n",
    "X3 = np.stack(X3)\n",
    "X3 = normalize(X3)\n",
    "y3 = dataset3_balanced.gender_str.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tre dataset vengono suddivisi in:\n",
    "- #### 70% train set\n",
    "- #### 30% test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PERCENTAGE = 0.3\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=TEST_PERCENTAGE, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=TEST_PERCENTAGE, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=TEST_PERCENTAGE, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio train e test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione viene creata una cartella per ogni dataset in cui salvare il train set e il test set. Questa operazione viene fatta per avere la possibilità di fare training sempre sugli stessi dati e valutare meglio l'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvataggio train e test set DATASET1 (CelebA)\n",
    "os.mkdir('dataset1')\n",
    "np.save('dataset1/X1_train', X1_train)\n",
    "np.save('dataset1/X1_test', X1_test)\n",
    "np.save('dataset1/y1_train', y1_train)\n",
    "np.save('dataset1/y1_test', y1_test)\n",
    "\n",
    "\n",
    "#Salvataggio train e test set DATASET2 (WheDataset)\n",
    "os.mkdir('dataset2')\n",
    "np.save('dataset2/X2_train', X2_train)\n",
    "np.save('dataset2/X2_test', X2_test)\n",
    "np.save('dataset2/y2_train', y2_train)\n",
    "np.save('dataset2/y2_test', y2_test)\n",
    "\n",
    "\n",
    "\n",
    "#Salvataggio train e test set DATASET3 (CelebA + WheDataset)\n",
    "os.mkdir('dataset3')\n",
    "np.save('dataset3/X3_train', X3_train)\n",
    "np.save('dataset3/X3_test', X3_test)\n",
    "np.save('dataset3/y3_train', y3_train)\n",
    "np.save('dataset3/y3_test', y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
